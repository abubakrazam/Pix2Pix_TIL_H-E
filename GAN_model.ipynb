{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modified version of the Pix2Pix GAN model tensorflow notebook from the link below\n",
    "\n",
    "## https://www.tensorflow.org/tutorials/generative/pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "# 1) Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) For file-handling operations (creating folders, finding images etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For File handling\n",
    "import os\n",
    "\n",
    "# To list all files in a folder given a path\n",
    "from glob import glob\n",
    "\n",
    "#Other system operations\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Tkinter uses interactive windows to select folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "\n",
    "\n",
    "#To discard a blank tkinter window that opens when the library is imported\n",
    "tkinter.Tk().withdraw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Deep Learning and basic machine learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning library - Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Numpy for mathematical operations\n",
    "import numpy as np\n",
    "\n",
    "#Scikit learn for train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Libraries for image processing operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:17.571065Z",
     "iopub.status.busy": "2021-01-08T04:20:17.570484Z",
     "iopub.status.idle": "2021-01-08T04:20:23.585125Z",
     "shell.execute_reply": "2021-01-08T04:20:23.584383Z"
    },
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "#matplotlib pyplot to plot/show images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Opencv for all image processing operations\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Dataset Path Selection\n",
    "\n",
    "## 2.1) Dialog box opens asking for the path of the dataset:\n",
    "\n",
    "Path should contain two folders (Case-sensitive):\n",
    "\n",
    "### Images (H&E images without CD3+ data - Input Image)\n",
    "\n",
    "\n",
    "### Masks (H&E images with CD3+ data - Reference/Ground Truth Image)\n",
    "\n",
    "(These names can be changed in the first cell in step 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=filedialog.askdirectory(title='path for whole dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Dataset Identification\n",
    "\n",
    "The following inputs along with the time of training help differentiate the pre-trained weights\n",
    "\n",
    "(These names can be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date of dataset creation\n",
    "date=\"Oct15_Same\"\n",
    "\n",
    "#Variation of dataset\n",
    "mask=\"Dataset1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Dataset Loading\n",
    "\n",
    "## 3.1) Loading and splitting dataset into training, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A python function to load data\n",
    "\n",
    "# Uses scikit-learn train-test split function\n",
    "\n",
    "# E.g: train_test_split(train_x, test_size=test_size, random_state=42), Random state kept constant for the randomisation\n",
    "# to be consistent whenever the same dataset is used\n",
    "\n",
    "def load_data(path, split=0.02): # Can change split percentage\n",
    "    \n",
    "    images = sorted(glob(os.path.join(path, \"Images/*\")))  #Imports H&E Images with no CD3+ data (Input image)\n",
    "    masks = sorted(glob(os.path.join(path, \"Masks/*\"))) # Imports H&E images with CD3+ data (Reference/Ground Truth Image)\n",
    "\n",
    "    total_size = len(images) # All images\n",
    "    valid_size = int(split * total_size) # Validation size\n",
    "    test_size = int(split * total_size) # Testing size\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42) #  Train-validation split (Input Image)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42) #  Train-validation split (Reference\n",
    "                                                                                                        #/Ground Truth Image)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42) #  Train-test split (Input Image)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42) #  Train-test split (Reference \n",
    "                                                                                                        #/Ground Truth Image)\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y) #Function return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1) Function Execution\n",
    "\n",
    "load_data takes the path obtained from step 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y)=load_data(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Reading images\n",
    "\n",
    "Images (Input and reference) are read in an unsigned integer 8 format, i.e., each pixel can have a value between 0-255 only (both values inclusive)\n",
    "\n",
    "If needed, the images are resized to a size of 256x256\n",
    "\n",
    "The images are converted to a 32-bit floating point tensor and the images are mapped between -1 to 1.\n",
    "\n",
    "* Images are converted from -1 to 1, as these are the outputs of the tanh activation function used by the generator U-Net Convolutional Neural Network (CNN)\n",
    "\n",
    "\n",
    "* Training dataset input function is separated from testing and validation to include future data augmentation approaches and to separate batch sizing parameters\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/image\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/io/decode_png\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1)  Load_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# All tensorflow image processing operations are used, as the given (input and reference/ground truth) images \n",
    "# are converted to tensors\n",
    "\n",
    "\n",
    "# a- input image\n",
    "\n",
    "# b- reference/ground truth image\n",
    "\n",
    "\n",
    "def load_train(a,b):\n",
    "    \n",
    "    #Reading input image from path using tensorflow's input-output module\n",
    "    \n",
    "    image1 = tf.io.read_file(a)\n",
    "    \n",
    "    #Decoding input image from path to unsigned integer 8 array using tensorflow's image module\n",
    "    \n",
    "    input_image = tf.image.decode_png(image1)\n",
    "    \n",
    "    #Resize input image to 256x256 if needed (Training dataset is expected to already be in 256x256 image patches)\n",
    "   \n",
    "    input_image=tf.image.resize(input_image, [256, 256],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    #Using tensorflow to cast input image data type from unsigned integer to floating point\n",
    "    \n",
    "    #Casting -> Converting a given variable from one data type (how information is represented) to another. No extra function\n",
    "    \n",
    "    input_image=tf.cast(input_image,tf.float32)\n",
    "    \n",
    "    # Mapping input image from 0 to 255 values to -1 to 1 values \n",
    "    \n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    \n",
    "    #Reading reference/ground-truth image from path using tensorflow's input-output module\n",
    "    \n",
    "    image2 = tf.io.read_file(b)\n",
    "    \n",
    "    #Decoding reference/ground-truth image from path to unsigned integer 8 array using tensorflow's image module\n",
    "    \n",
    "    real_image = tf.image.decode_png(image2)\n",
    "    \n",
    "     #Resize reference/ground-truth image to 256x256 if needed (Training dataset is expected to already be in 256x256 \n",
    "                                                                                                            #image patches)\n",
    "\n",
    "    real_image=tf.image.resize(real_image, [256, 256],\n",
    "                      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    #Using tensorflow to cast reference/ground-truth image data type from unsigned integer to floating point\n",
    "    \n",
    "    #Casting -> Converting a given variable from one data type (how information is represented) to another. No extra function\n",
    "\n",
    "    real_image=tf.cast(real_image,tf.float32)\n",
    "    \n",
    "    # Mapping reference/ground-truth image from 0 to 255 values to -1 to 1 values \n",
    "\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "    \n",
    "    #Function return\n",
    "    \n",
    "    return input_image, real_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2) Validation/Test Dataset Loading\n",
    "\n",
    "* For image inference, i.e., when using an external dataset for validation purposes, there is no need to read a reference image\n",
    "\n",
    "The condtn parameter in load_valid needs to be set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valid(a,b=0,condtn=True):\n",
    "    \n",
    "    #Reading input image from path using tensorflow's input-output module\n",
    "    \n",
    "    image1 = tf.io.read_file(a)\n",
    "    \n",
    "    #Decoding input image from path to unsigned integer 8 array using tensorflow's image module\n",
    "    \n",
    "    input_image = tf.image.decode_png(image1,channels=3)\n",
    "    \n",
    "    #Padding (adding) empty regions of input image less than 256x256 with zeros\n",
    "    \n",
    "    if(input_image.shape!=([256,256,3])):\n",
    "        input_image=tf.image.pad_to_bounding_box(input_image,0,0,256,256)\n",
    "\n",
    "    #Using tensorflow to cast input image data type from unsigned integer to floating point\n",
    "    \n",
    "    #Casting -> Converting a given variable from one data type (how information is represented) to another. No extra function\n",
    "        \n",
    "        \n",
    "    input_image=tf.cast(input_image,tf.float32)\n",
    "    \n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    \n",
    "    if(condtn==True): # Same Function can be used for H&E images only - for inference purposes (no ground truth data present)\n",
    "        \n",
    "        #Reading reference/ground-truth image from path using tensorflow's input-output library\n",
    "        \n",
    "        image2 = tf.io.read_file(b)\n",
    "        \n",
    "        #Decoding reference/ground-truth image from path to unsigned integer 8 array using tensorflow's image library\n",
    "        \n",
    "        real_image = tf.image.decode_png(image2)\n",
    "        \n",
    "         #Padding (adding) empty regions of input image less than 256x256 with zeros\n",
    "    \n",
    "        if(real_image.shape!=([256,256,3])):\n",
    "            real_image=tf.image.pad_to_bounding_box(real_image,0,0,256,256)\n",
    "            \n",
    "        #Using tensorflow to cast input image data type from unsigned integer to floating point\n",
    "    \n",
    "        #Casting -> Converting a given variable from one data type (how information is represented) to another. \n",
    "        #No extra function\n",
    "         \n",
    "\n",
    "        real_image=tf.cast(real_image,tf.float32)\n",
    "    \n",
    "        # Mapping reference/ground-truth image from 0 to 255 values to -1 to 1 values \n",
    "        \n",
    "        real_image = (real_image / 127.5) - 1\n",
    "\n",
    "        return input_image, real_image # Return type 1-> Input and reference/ground-truth image\n",
    "    else:\n",
    "        return input_image # Return type 2-> Input image only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3) Tensorflow Dataset Creation with tensor slices\n",
    "\n",
    "Datasets are created using the \"tensor slices\" method\n",
    "\n",
    "https://www.tensorflow.org/guide/tensor_slicing\n",
    "\n",
    "This calls the above functions per batch, leading to lesser GPU (Graphics Processing Unit) usage, compared to all the dataset being in the GPU memory at the same time using the \"map\" function\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\n",
    "\n",
    "\n",
    "The dataset is also shuffled using the \"shuffle\" function with a buffer size\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 400 #For shuffling a dataset\n",
    "\n",
    "\n",
    "BATCH_SIZE = 350 #25 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Creation\n",
    "\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_dataset=train_dataset.shuffle(BUFFER_SIZE) #The Dataset is shuffled with a set buffer size\n",
    "train_dataset=train_dataset.map(load_train)\n",
    "train_dataset=train_dataset.batch(BATCH_SIZE) \n",
    "\n",
    "# Validation Dataset Creation\n",
    "\n",
    "valid_dataset=tf.data.Dataset.from_tensor_slices((valid_x,valid_y))\n",
    "valid_dataset=valid_dataset.map(load_valid)\n",
    "valid_dataset=valid_dataset.batch(1) # Batch size is 1\n",
    "\n",
    "# Testing Dataset Creation\n",
    "\n",
    "test_dataset=tf.data.Dataset.from_tensor_slices((test_x,test_y))\n",
    "test_dataset=test_dataset.map(load_valid)\n",
    "test_dataset=test_dataset.batch(1) #Batch Size is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "# 4) U-Net Generator Creation\n",
    "\n",
    "  * The architecture of generator is a modified U-Net.\n",
    "  * Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "  * Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
    "  * There are skip connections between the encoder and decoder (as in U-Net)\n",
    "  \n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=256\n",
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:33.722873Z",
     "iopub.status.busy": "2021-01-08T04:20:33.722285Z",
     "iopub.status.idle": "2021-01-08T04:20:33.724196Z",
     "shell.execute_reply": "2021-01-08T04:20:33.723776Z"
    },
    "id": "3R09ATE_SH9P"
   },
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:33.727924Z",
     "iopub.status.busy": "2021-01-08T04:20:33.727357Z",
     "iopub.status.idle": "2021-01-08T04:20:35.552302Z",
     "shell.execute_reply": "2021-01-08T04:20:35.552698Z"
    },
    "id": "a6_uCZCppTh7"
   },
   "outputs": [],
   "source": [
    "down_model = downsample(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:35.559135Z",
     "iopub.status.busy": "2021-01-08T04:20:35.558534Z",
     "iopub.status.idle": "2021-01-08T04:20:35.560901Z",
     "shell.execute_reply": "2021-01-08T04:20:35.560441Z"
    },
    "id": "nhgDsHClSQzP"
   },
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:35.566976Z",
     "iopub.status.busy": "2021-01-08T04:20:35.566287Z",
     "iopub.status.idle": "2021-01-08T04:20:35.622232Z",
     "shell.execute_reply": "2021-01-08T04:20:35.622612Z"
    },
    "id": "mz-ahSdsq0Oc"
   },
   "outputs": [],
   "source": [
    "up_model = upsample(3, 4)\n",
    "up_result = up_model(down_result)\n",
    "print (up_result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:35.631772Z",
     "iopub.status.busy": "2021-01-08T04:20:35.631160Z",
     "iopub.status.idle": "2021-01-08T04:20:35.633455Z",
     "shell.execute_reply": "2021-01-08T04:20:35.632846Z"
    },
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "  size=4\n",
    "  inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
    "\n",
    "  down_stack = [\n",
    "    downsample(64, size, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "    downsample(128,size), # (bs, 64, 64, 128)\n",
    "    downsample(256, size), # (bs, 32, 32, 256)\n",
    "    downsample(512, size), # (bs, 16, 16, 512)\n",
    "    downsample(512, size), # (bs, 8, 8, 512)\n",
    "    downsample(512, size), # (bs, 4, 4, 512)\n",
    "    downsample(512, size), # (bs, 2, 2, 512)\n",
    "    downsample(512, size), # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(512, size, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "    upsample(512, size, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "    upsample(512, size, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "    upsample(512, size), # (bs, 16, 16, 1024)\n",
    "    upsample(256, size), # (bs, 32, 32, 512)\n",
    "    upsample(128, size), # (bs, 64, 64, 256)\n",
    "    upsample(64, size), # (bs, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, size,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:35.640676Z",
     "iopub.status.busy": "2021-01-08T04:20:35.640103Z",
     "iopub.status.idle": "2021-01-08T04:20:36.448140Z",
     "shell.execute_reply": "2021-01-08T04:20:36.448516Z"
    },
    "id": "dIbRPFzjmV85"
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpDPEQXIAiQO"
   },
   "source": [
    "* **Generator loss**\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
    "  * The [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSZbDgESHIV6"
   },
   "source": [
    "The training procedure for the generator is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:36.816276Z",
     "iopub.status.busy": "2021-01-08T04:20:36.815684Z",
     "iopub.status.idle": "2021-01-08T04:20:36.817543Z",
     "shell.execute_reply": "2021-01-08T04:20:36.817893Z"
    },
    "id": "cyhxTuvJyIHV"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:36.822042Z",
     "iopub.status.busy": "2021-01-08T04:20:36.821488Z",
     "iopub.status.idle": "2021-01-08T04:20:36.823067Z",
     "shell.execute_reply": "2021-01-08T04:20:36.823463Z"
    },
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlB-XMY5Awj9"
   },
   "source": [
    "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTKZfoaoEF22"
   },
   "source": [
    "# 5) Discriminator Design\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN).\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake.\n",
    "    * We concatenate these 2 inputs together in the code (`tf.concat([inp, tar], axis=-1)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:36.830842Z",
     "iopub.status.busy": "2021-01-08T04:20:36.830275Z",
     "iopub.status.idle": "2021-01-08T04:20:36.831801Z",
     "shell.execute_reply": "2021-01-08T04:20:36.832146Z"
    },
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  size=4\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, size, False)(x) # (bs, 128, 128, 64)\n",
    "  down2 = downsample(128, size)(down1) # (bs, 64, 64, 128)\n",
    "  down3 = downsample(256, size)(down2) # (bs, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, size, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, size, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:36.840117Z",
     "iopub.status.busy": "2021-01-08T04:20:36.839158Z",
     "iopub.status.idle": "2021-01-08T04:20:37.077914Z",
     "shell.execute_reply": "2021-01-08T04:20:37.078297Z"
    },
    "id": "YHoUui4om-Ev"
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.084770Z",
     "iopub.status.busy": "2021-01-08T04:20:37.084057Z",
     "iopub.status.idle": "2021-01-08T04:20:37.308424Z",
     "shell.execute_reply": "2021-01-08T04:20:37.308827Z"
    },
    "id": "gDkA05NE6QMs"
   },
   "outputs": [],
   "source": [
    "disc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)\n",
    "plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOqg1dhUAWoD"
   },
   "source": [
    "**Discriminator loss**\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.312862Z",
     "iopub.status.busy": "2021-01-08T04:20:37.312307Z",
     "iopub.status.idle": "2021-01-08T04:20:37.314262Z",
     "shell.execute_reply": "2021-01-08T04:20:37.313852Z"
    },
    "id": "Q1Xbz5OaLj5C"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.318115Z",
     "iopub.status.busy": "2021-01-08T04:20:37.317551Z",
     "iopub.status.idle": "2021-01-08T04:20:37.319661Z",
     "shell.execute_reply": "2021-01-08T04:20:37.319202Z"
    },
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    #print(total_disc_loss)\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ede4p2YELFa"
   },
   "source": [
    "The training procedure for the discriminator is shown below.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS9sHa-1BoAF"
   },
   "source": [
    "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FMYgY_mPfTi"
   },
   "source": [
    "## 4.2) Define the Optimizers and Checkpoint-saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.323707Z",
     "iopub.status.busy": "2021-01-08T04:20:37.323167Z",
     "iopub.status.idle": "2021-01-08T04:20:37.324858Z",
     "shell.execute_reply": "2021-01-08T04:20:37.325246Z"
    },
    "id": "lbHFNexF0x6O"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.329646Z",
     "iopub.status.busy": "2021-01-08T04:20:37.329055Z",
     "iopub.status.idle": "2021-01-08T04:20:37.330724Z",
     "shell.execute_reply": "2021-01-08T04:20:37.331077Z"
    },
    "id": "WJnftd5sQsv6"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/'+date+\"/\"+mask\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_prefix, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## 5) Generate Images\n",
    "\n",
    "Write a function to plot some images during training.\n",
    "\n",
    "* We pass images from the test dataset to the generator.\n",
    "* The generator will then translate the input image into the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rb0QQFHF-JfS"
   },
   "source": [
    "Note: The `training=True` is intentional here since\n",
    "we want the batch statistics while running the model\n",
    "on the test dataset. If we use training=False, we will get\n",
    "the accumulated statistics learned from the training dataset\n",
    "(which we don't want)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.336597Z",
     "iopub.status.busy": "2021-01-08T04:20:37.336010Z",
     "iopub.status.idle": "2021-01-08T04:20:37.337634Z",
     "shell.execute_reply": "2021-01-08T04:20:37.338041Z"
    },
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ref for style_transfer', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLKOG55MErD0"
   },
   "source": [
    "## 6) Training\n",
    "\n",
    "* For each example input generate an output.\n",
    "* The discriminator receives the input_image and the generated image as the first input. The second input is the input_image and the target_image.\n",
    "* Next, we calculate the generator and the discriminator loss.\n",
    "* Then, we calculate the gradients of loss with respect to both the generator and the discriminator variables(inputs) and apply those to the optimizer.\n",
    "* Then log the losses to TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.642166Z",
     "iopub.status.busy": "2021-01-08T04:20:37.641548Z",
     "iopub.status.idle": "2021-01-08T04:20:37.643159Z",
     "shell.execute_reply": "2021-01-08T04:20:37.643500Z"
    },
    "id": "NS2GWywBbAWo"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.647343Z",
     "iopub.status.busy": "2021-01-08T04:20:37.646775Z",
     "iopub.status.idle": "2021-01-08T04:20:37.648933Z",
     "shell.execute_reply": "2021-01-08T04:20:37.649302Z"
    },
    "id": "xNNMDBNH12q-"
   },
   "outputs": [],
   "source": [
    "# Saving Directories with different dates\n",
    "import datetime\n",
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.656320Z",
     "iopub.status.busy": "2021-01-08T04:20:37.655689Z",
     "iopub.status.idle": "2021-01-08T04:20:37.657339Z",
     "shell.execute_reply": "2021-01-08T04:20:37.657687Z"
    },
    "id": "KBKUV2sKXDbY"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    gen_output = generator(input_image, training=True)\n",
    "    \n",
    "    ps=tf.reduce_mean(tf.image.psnr(gen_output,target, max_val=1.0))\n",
    "\n",
    "    ss=tf.reduce_mean(tf.image.ssim(gen_output,target, 1.0))\n",
    "\n",
    "    disc_real_output = discriminator([input_image, target], training=True)\n",
    "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "  \n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
    "    \n",
    "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
    "    \n",
    "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
    "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
    "    #Additional Metrics maybe used as an alternative to L1-Loss\n",
    "    tf.summary.scalar('ssim', ss, step=epoch) \n",
    "    tf.summary.scalar('psnr', ps, step=epoch) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx7s-vBHFKdh"
   },
   "source": [
    "The actual training loop:\n",
    "\n",
    "* Iterates over the number of epochs.\n",
    "* On each epoch it clears the display, and runs `generate_images` to show it's progress.\n",
    "* On each epoch it iterates over the training dataset, printing a '.' for each example.\n",
    "* It saves a checkpoint every 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.663798Z",
     "iopub.status.busy": "2021-01-08T04:20:37.663231Z",
     "iopub.status.idle": "2021-01-08T04:20:37.665189Z",
     "shell.execute_reply": "2021-01-08T04:20:37.664728Z"
    },
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "        for example_input, example_target in test_ds.take(3):\n",
    "            generate_images(generator, example_input, example_target)\n",
    "        \n",
    "        print(\"Epoch: \", epoch)\n",
    "\n",
    "    # Train\n",
    "        for n, (input_image, target) in train_ds.enumerate():\n",
    "            print('.', end='')\n",
    "   \n",
    "            train_step(input_image, target, epoch)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "                \n",
    "        manager.save()\n",
    "\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                        time.time()-start))\n",
    "    #checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:59:43.787042Z",
     "iopub.status.busy": "2021-01-08T04:59:43.775471Z",
     "iopub.status.idle": "2021-01-08T04:59:44.376657Z",
     "shell.execute_reply": "2021-01-08T04:59:44.376098Z"
    },
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wozqyTh2wmCu"
   },
   "source": [
    "This training loop saves logs you can easily view in TensorBoard to monitor the training progress. Working locally you would launch a separate tensorboard process. In a notebook, if you want to monitor with TensorBoard it's easiest to launch the viewer before starting the training.\n",
    "\n",
    "To launch the viewer paste the following into a code-cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot22ujrlLhOd"
   },
   "outputs": [],
   "source": [
    "#docs_infra: no_execute\n",
    "#!kill 1787\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}  \n",
    "#%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0-8Bzg22ox"
   },
   "source": [
    "Now run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.668541Z",
     "iopub.status.busy": "2021-01-08T04:20:37.667951Z",
     "iopub.status.idle": "2021-01-08T04:59:43.521945Z",
     "shell.execute_reply": "2021-01-08T04:59:43.522342Z"
    },
    "id": "a1zZmKmvOH85",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(train_dataset, EPOCHS, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz80bY3aQ1VZ"
   },
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:59:43.592464Z",
     "iopub.status.busy": "2021-01-08T04:59:43.591655Z",
     "iopub.status.idle": "2021-01-08T04:59:43.771440Z",
     "shell.execute_reply": "2021-01-08T04:59:43.771803Z"
    },
    "id": "HSSm4kfvJiqv"
   },
   "outputs": [],
   "source": [
    "!ls {checkpoint_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:59:43.787042Z",
     "iopub.status.busy": "2021-01-08T04:59:43.775471Z",
     "iopub.status.idle": "2021-01-08T04:59:44.376657Z",
     "shell.execute_reply": "2021-01-08T04:59:44.376098Z"
    },
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:59:44.381205Z",
     "iopub.status.busy": "2021-01-08T04:59:44.380602Z",
     "iopub.status.idle": "2021-01-08T04:59:45.738473Z",
     "shell.execute_reply": "2021-01-08T04:59:45.738911Z"
    },
    "id": "KUgSnmy2nqSP"
   },
   "source": [
    "## Testing trained model on datasets.\n",
    "\n",
    "Tkinter is used to make this step interactive. It asks for the path of the input and output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train=filedialog.askdirectory(title='path for train_results')\n",
    "P_train_gt=os.path.join(p_train,\"gt\")\n",
    "P_train_pred=os.path.join(p_train,\"pred\")\n",
    "P_train_img=os.path.join(p_train,\"img\")\n",
    "os.makedirs(P_train_img,exist_ok=True)\n",
    "os.makedirs(P_train_gt,exist_ok=True)\n",
    "os.makedirs(P_train_pred,exist_ok=True)\n",
    "i=0\n",
    "for x,y in train_dataset:\n",
    "    for a,b in zip(x,y):\n",
    "        v=generator(np.expand_dims(a,axis=0))\n",
    "        img1=v[0]*0.5+0.5\n",
    "        img2=b*0.5+0.5\n",
    "        img3=a*0.5+0.5\n",
    "        plt.imsave(P_train_pred+\"/\"+f\"{i:06d}\"+\".png\",img1.numpy())\n",
    "        plt.imsave(P_train_gt+\"/\"+f\"{i:06d}\"+\".png\",img2.numpy())\n",
    "        plt.imsave(P_train_img+\"/\"+f\"{i:06d}\"+\".png\",img3.numpy())\n",
    "        print(i, end= \" \")\n",
    "        i=i+1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
