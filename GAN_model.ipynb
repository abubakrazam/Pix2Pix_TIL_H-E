{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a modified version of the Pix2Pix GAN model tensorflow notebook from the link below\n",
    "\n",
    "## https://www.tensorflow.org/tutorials/generative/pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Chapter 1: Importing Tensorflow and Other Libraries](#chapter1)\n",
    "    * [Section 1.1: For file-handling operations (creating folders, finding images etc.)](#section_1_1)\n",
    "    * [Section 1.2: Tkinter (for interactive folder path finding)](#section_1_2)\n",
    "    * [Section 1.3: Deep learning and basic machine learning libraries](#section_1_3)\n",
    "    * [Section 1.4: Libraries for image processing operations](#section_1_4)\n",
    "* [Chapter 2: Dataset Path Selection](#chapter2)\n",
    "    * [Section 2.1: Path selection dialog box](#section_2_1)\n",
    "    * [Section 2.2: Dataset identification](#section_2_2)\n",
    "* [Chapter 3: Dataset Loading](#chapter3)\n",
    "    * [Section 3.1: Dataset division](#section_3_1)\n",
    "        * [Sub Section 3.1.1: Function definition](#section_3_1_1)\n",
    "        * [Sub Section 3.1.2: Function execution](#section_3_1_2)\n",
    "    * [Section 3.2: Reading images](#section_3_2)\n",
    "        * [Sub Section 3.2.1: Function definition for loading training dataset](#section_3_2_1)\n",
    "        * [Sub Section 3.2.2: Function definition for loading testing/validation dataset](#section_3_2_2)\n",
    "    * [Section 3.3: Tensorflow dataset creation with tensor slices](#section_3_3)\n",
    "* [Chapter 4: Pix2Pix GAN Model Creation](#chapter4)\n",
    "    * [Section 4.1: Setting common conditions](#section_4_1)\n",
    "        * [Sub Section 4.1.1: Patch size and output channel setting](#section_4_1_1)\n",
    "        * [Sub Section 4.1.2: Binary cross-entropy loss function definition](#section_4_1_2)\n",
    "    * [Section 4.2: Generator model creation](#section_4_2)\n",
    "        * [Sub Section 4.2.1: Encoder definition](#section_4_2_1)\n",
    "        * [Sub Section 4.2.2: Decoder definition](#section_4_2_2)\n",
    "        * [Sub Section 4.2.3: U-Net model creation](#section_4_2_3)\n",
    "        * [Sub Section 4.2.4: Generator loss](#section_4_2_4)\n",
    "    * [Section 4.3: Discriminator model creation](#section_4_3)\n",
    "        * [Sub Section 4.3.1: Model definition](#section_4_3_1)\n",
    "        * [Sub Section 4.3.2: Discriminator loss definition](#section_4_3_2)\n",
    "* [Chapter 5: Model Training](#chapter5)\n",
    "    * [Section 5.1: Optimizer definition](#section_5_1)\n",
    "    * [Section 5.2: Checkpoint definition](#section_5_2)\n",
    "        * [Sub Section 5.2.1: Checkpoint folder naming](#section_5_2_1)\n",
    "        * [Sub Section 5.2.2: Checkpoint setup using tensorflow functions](#section_5_2_2)\n",
    "    * [Section 5.3: Model training](#section_5_3)\n",
    "        * [Sub Section 5.3.1: Training function definition](#section_5_3_1)\n",
    "        * [Sub Section 5.3.2: Tensorboard setup](#section_5_3_2)\n",
    "        * [Sub Section 5.3.3: Fit function definition](#section_5_3_3)\n",
    "        * [Sub Section 5.3.4: Setting epochs and loading checkpoints (if any)](#section_5_3_4)\n",
    "        * [Sub Section 5.3.5: Running the \"fit\" function](#section_5_3_5)\n",
    "* [Chapter 6: Additional functions](#chapter6)\n",
    "    * [Section 6.1: Displaying generated images](#section_6_1)\n",
    "    * [Section 6.2: Model inference](#section_6_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Importing Tensorflow and Other Libraries <a class=\"anchor\" id=\"chapter1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "### Section 1.1: For file-handling operations (creating folders, finding images etc.) <a class=\"anchor\" id=\"section_1_1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For File handling\n",
    "import os\n",
    "\n",
    "# To list all files in a folder given a path\n",
    "from glob import glob\n",
    "\n",
    "#Other system operations\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: Tkinter (for interactive folder path finding) <a class=\"anchor\" id=\"section_1_2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tkinter\n",
    "from tkinter import filedialog\n",
    "\n",
    "\n",
    "#To discard a blank tkinter window that opens when the library is imported\n",
    "tkinter.Tk().withdraw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3: Deep learning and basic machine learning libraries <a class=\"anchor\" id=\"section_1_3\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning library - Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "#Numpy for mathematical operations\n",
    "import numpy as np\n",
    "\n",
    "#Scikit learn for train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4: Libraries for image processing operations <a class=\"anchor\" id=\"section_1_4\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:17.571065Z",
     "iopub.status.busy": "2021-01-08T04:20:17.570484Z",
     "iopub.status.idle": "2021-01-08T04:20:23.585125Z",
     "shell.execute_reply": "2021-01-08T04:20:23.584383Z"
    },
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "#matplotlib pyplot to plot/show images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Opencv for all image processing operations\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Dataset Path Selection <a class=\"anchor\" id=\"chapter2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1: Path selection dialog box <a class=\"anchor\" id=\"section_2_1\"></a>\n",
    "\n",
    "* Path should contain two folders (Case-sensitive):\n",
    "    * Images (H&E images without CD3+ data - Input Image)\n",
    "    * Masks (H&E images with CD3+ data - Reference/Ground Truth Image)\n",
    "\n",
    "* These names can be changed in the first cell in section 3.1 \n",
    "[Go there](#section_3_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=filedialog.askdirectory(title='path for whole dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Dataset identification <a class=\"anchor\" id=\"section_2_2\"></a>\n",
    "\n",
    "\n",
    "* The following inputs along with the time of training help differentiate the pre-trained weights\n",
    "\n",
    "* These names can be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date of dataset creation\n",
    "date=\"Oct15_Same\"\n",
    "\n",
    "#Variation of dataset\n",
    "mask=\"Dataset1\"\n",
    "\n",
    "#Paths for generator and discriminator weights\n",
    "path_gen=\"Model_weights/\"+date+\"/Generator\"\n",
    "path_dis=\"Model_weights/\"+date+\"/Discriminator\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Dataset Loading <a class=\"anchor\" id=\"chapter3\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1: Dataset division <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "Dataset is divided into, \"train\", \"test\" and \"valid\" subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 3.1.1: Function definition <a class=\"anchor\" id=\"section_3_1_1\"></a>\n",
    "* Uses scikit-learn train-test split function\n",
    "* Random state kept constant for the randomisation to be consistent whenever the same dataset is used\n",
    "* Function Inputs:\n",
    "    * path: folder path of images \n",
    "    [(See path definition in Section 2.1)](#section_2_1)\n",
    "    * split: Split size (default is 2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, split=0.02): # Can change split percentage\n",
    "    \n",
    "    images = sorted(glob(os.path.join(path, \"Images/*\")))  #Imports H&E Images with no CD3+ data (Input image)\n",
    "    masks = sorted(glob(os.path.join(path, \"Masks/*\"))) # Imports H&E images with CD3+ data (Reference/Ground Truth Image)\n",
    "\n",
    "    total_size = len(images) # All images\n",
    "    valid_size = int(split * total_size) # Validation size\n",
    "    test_size = int(split * total_size) # Testing size\n",
    "\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42) #  Train-validation split (Input Image)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42) #  Train-validation split (Reference\n",
    "                                                                                                        #/Ground Truth Image)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42) #  Train-test split (Input Image)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42) #  Train-test split (Reference \n",
    "                                                                                                        #/Ground Truth Image)\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y) #Function return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 3.1.2: Function execution <a class=\"anchor\" id=\"section_3_1_2\"></a>\n",
    "\n",
    "* load_data takes the path obtained from section 2.1 \n",
    "[Go there](#section_2_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y)=load_data(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2: Reading images <a class=\"anchor\" id=\"section_3_2\"></a>\n",
    "\n",
    "\n",
    "* Images (Input and reference) are read in an unsigned integer 8 format, i.e., each pixel can have a value between 0-255 only (both values inclusive)\n",
    "\n",
    "    * If needed, the images are resized to a size of 256x256\n",
    "\n",
    "* The images are converted to a 32-bit floating point tensor and the images are mapped between -1 to 1.\n",
    "\n",
    "    * Images are converted from -1 to 1, as these are the outputs of the tanh activation function used by the generator U-Net Convolutional Neural Network (CNN)\n",
    "\n",
    "\n",
    "* Training dataset input function is separated from testing and validation to include future data augmentation approaches and to separate batch sizing parameters\n",
    "\n",
    "* Additional links to tensorflow function definitions\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/image\n",
    "\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/io/decode_png\n",
    "\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 3.2.1: Function definition for loading training dataset <a class=\"anchor\" id=\"section_3_2_1\"></a> \n",
    "\n",
    "* All tensorflow image processing operations are used, as the given (input and reference/ground truth) images are converted to tensors\n",
    "* Function Inputs\n",
    "    * a- input image\n",
    "    * b- reference/ground truth image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(a,b):\n",
    "    \n",
    "    #Reading input image from path using tensorflow's input-output module\n",
    "    \n",
    "    image1 = tf.io.read_file(a)\n",
    "    \n",
    "    #Decoding input image from path to unsigned integer 8 array using tensorflow's image module\n",
    "    \n",
    "    input_image = tf.image.decode_png(image1)\n",
    "    \n",
    "    #Resize input image to 256x256 if needed (Training dataset is expected to already be in 256x256 image patches)\n",
    "   \n",
    "    input_image=tf.image.resize(input_image, [256, 256],\n",
    "                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    #Using tensorflow to cast input image data type from unsigned integer to floating point\n",
    "    \n",
    "    #Casting -> Converting a given variable from one data type (how information is represented) to another. No extra function\n",
    "    \n",
    "    input_image=tf.cast(input_image,tf.float32)\n",
    "    \n",
    "    # Mapping input image from 0 to 255 values to -1 to 1 values \n",
    "    \n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    \n",
    "    #Reading reference/ground-truth image from path using tensorflow's input-output module\n",
    "    \n",
    "    image2 = tf.io.read_file(b)\n",
    "    \n",
    "    #Decoding reference/ground-truth image from path to unsigned integer 8 array using tensorflow's image module\n",
    "    \n",
    "    real_image = tf.image.decode_png(image2)\n",
    "    \n",
    "     #Resize reference/ground-truth image to 256x256 if needed (Training dataset is expected to already be in 256x256 \n",
    "                                                                                                            #image patches)\n",
    "\n",
    "    real_image=tf.image.resize(real_image, [256, 256],\n",
    "                      method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    #Using tensorflow to cast reference/ground-truth image data type from unsigned integer to floating point\n",
    "    \n",
    "    #Casting -> Converting a given variable from one data type (how information is represented) to another. No extra function\n",
    "\n",
    "    real_image=tf.cast(real_image,tf.float32)\n",
    "    \n",
    "    # Mapping reference/ground-truth image from 0 to 255 values to -1 to 1 values \n",
    "\n",
    "    real_image = (real_image / 127.5) - 1\n",
    "    \n",
    "    #Function return\n",
    "    \n",
    "    return input_image, real_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 3.2.2: Function Definition for loading testing/validation dataset <a class=\"anchor\" id=\"section_3_2_2\"></a>\n",
    "\n",
    "* There is no need to put images in batches, or to include any data augmentation for such datasets\n",
    "* For image inference, i.e., when using an external dataset for validation purposes, there is no need to read a reference image\n",
    "\n",
    "    * The condtn parameter in load_valid needs to be set to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valid(a,b=0,condtn=True):\n",
    "    \n",
    "    #Reading input image from path using tensorflow's input-output module\n",
    "    \n",
    "    image1 = tf.io.read_file(a)\n",
    "    \n",
    "    #Decoding input image from path to unsigned integer 8 array using tensorflow's image module\n",
    "    \n",
    "    input_image = tf.image.decode_png(image1,channels=3)\n",
    "    \n",
    "    #Padding (adding) empty regions of input image less than 256x256 with zeros\n",
    "    \n",
    "    if(input_image.shape!=([256,256,3])):\n",
    "        input_image=tf.image.pad_to_bounding_box(input_image,0,0,256,256)\n",
    "\n",
    "    #Using tensorflow to cast input image data type from unsigned integer to floating point\n",
    "    \n",
    "    #Casting -> Converting a given variable from one data type (how information is represented) to another. No extra function\n",
    "        \n",
    "        \n",
    "    input_image=tf.cast(input_image,tf.float32)\n",
    "    \n",
    "    input_image = (input_image / 127.5) - 1\n",
    "    \n",
    "    if(condtn==True): # Same Function can be used for H&E images only - for inference purposes (no ground truth data present)\n",
    "        \n",
    "        #Reading reference/ground-truth image from path using tensorflow's input-output library\n",
    "        \n",
    "        image2 = tf.io.read_file(b)\n",
    "        \n",
    "        #Decoding reference/ground-truth image from path to unsigned integer 8 array using tensorflow's image library\n",
    "        \n",
    "        real_image = tf.image.decode_png(image2)\n",
    "        \n",
    "         #Padding (adding) empty regions of input image less than 256x256 with zeros\n",
    "    \n",
    "        if(real_image.shape!=([256,256,3])):\n",
    "            real_image=tf.image.pad_to_bounding_box(real_image,0,0,256,256)\n",
    "            \n",
    "        #Using tensorflow to cast input image data type from unsigned integer to floating point\n",
    "    \n",
    "        #Casting -> Converting a given variable from one data type (how information is represented) to another. \n",
    "        #No extra function\n",
    "         \n",
    "\n",
    "        real_image=tf.cast(real_image,tf.float32)\n",
    "    \n",
    "        # Mapping reference/ground-truth image from 0 to 255 values to -1 to 1 values \n",
    "        \n",
    "        real_image = (real_image / 127.5) - 1\n",
    "\n",
    "        return input_image, real_image # Return type 1-> Input and reference/ground-truth image\n",
    "    else:\n",
    "        return input_image # Return type 2-> Input image only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.3: Tensorflow Dataset Creation with tensor slices <a class=\"anchor\" id=\"section_3_3\"></a>\n",
    "\n",
    "* Datasets are created using the \"tensor slices\" method\n",
    "    * This calls the above functions per batch, leading to lesser GPU (Graphics Processing Unit) usage, compared to all the dataset being in the GPU memory at the same time using the \"map\" function\n",
    "* The dataset is also shuffled using the \"shuffle\" function with a buffer size\n",
    "* Additional links to tensorflow function definitions\n",
    "    * https://www.tensorflow.org/guide/tensor_slicing\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 400 #For shuffling a dataset\n",
    "BATCH_SIZE = 350 #25 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset Creation\n",
    "\n",
    "train_dataset=tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "train_dataset=train_dataset.shuffle(BUFFER_SIZE) #The Dataset is shuffled with a set buffer size\n",
    "train_dataset=train_dataset.map(load_train)\n",
    "train_dataset=train_dataset.batch(BATCH_SIZE) \n",
    "\n",
    "# Validation Dataset Creation\n",
    "\n",
    "valid_dataset=tf.data.Dataset.from_tensor_slices((valid_x,valid_y))\n",
    "valid_dataset=valid_dataset.map(load_valid)\n",
    "valid_dataset=valid_dataset.batch(1) # Batch size is 1\n",
    "\n",
    "# Testing Dataset Creation\n",
    "\n",
    "test_dataset=tf.data.Dataset.from_tensor_slices((test_x,test_y))\n",
    "test_dataset=test_dataset.map(load_valid)\n",
    "test_dataset=test_dataset.batch(1) #Batch Size is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THY-sZMiQ4UV"
   },
   "source": [
    "## Chapter 4: Pix2Pix GAN Model Creation <a class=\"anchor\" id=\"chapter4\"></a>\n",
    "Consists of Generator and Discriminator model building and loss function assignment  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.1: Setting common conditions <a class=\"anchor\" id=\"section_4_1\"></a>\n",
    "* Setting common condtions related to image sizes and loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 4.1.1: Patch size and output channel setting <a class=\"anchor\" id=\"section_4_1_1\"></a>\n",
    "* Patch size is 256*256\n",
    "* Using RGB channel images, the number of output channels would be 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE=256\n",
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 4.1.2: Binary cross-entropy loss function definition <a class=\"anchor\" id=\"section_4_1_2\"></a>\n",
    "* Binary Cross-Entropy loss is used both in the generator and the discriminator\n",
    "* Tensorflow loss definition link\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2: Generator model creation <a class=\"anchor\" id=\"section_4_2\"></a>\n",
    "  * The architecture of generator is a modified U-Net, which contains an encoder and decoder.\n",
    "  * There are skip connections between the encoder and decoder (as in U-Net)\n",
    "  * Additional links to tensorflow definitions\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D \n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization \n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\n",
    "    * https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 4.2.1: Encoder definition <a class=\"anchor\" id=\"section_4_2_1\"></a>\n",
    "* Each block in the encoder is (Conv -> Batchnorm -> Leaky ReLU)\n",
    "* Sometimes, the encoder model is termed as a downscaling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:33.722873Z",
     "iopub.status.busy": "2021-01-08T04:20:33.722285Z",
     "iopub.status.idle": "2021-01-08T04:20:33.724196Z",
     "shell.execute_reply": "2021-01-08T04:20:33.723776Z"
    },
    "id": "3R09ATE_SH9P"
   },
   "outputs": [],
   "source": [
    "def encoder(filters, size, apply_batchnorm=True):\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "down_model = encoder(3, 4)\n",
    "down_result = down_model(tf.expand_dims(inp, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 4.2.2: Decoder definition <a class=\"anchor\" id=\"section_4_2_2\"></a>\n",
    "* Each block in the decoder is (Transposed Conv -> Batchnorm -> Dropout(applied to the first 3 blocks) -> ReLU)\n",
    "* Sometimes, the decoder model is termed as a upscaling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:35.559135Z",
     "iopub.status.busy": "2021-01-08T04:20:35.558534Z",
     "iopub.status.idle": "2021-01-08T04:20:35.560901Z",
     "shell.execute_reply": "2021-01-08T04:20:35.560441Z"
    },
    "id": "nhgDsHClSQzP"
   },
   "outputs": [],
   "source": [
    "def decoder(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n",
    "up_model = decoder(3, 4)\n",
    "up_result = up_model(down_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:35.566976Z",
     "iopub.status.busy": "2021-01-08T04:20:35.566287Z",
     "iopub.status.idle": "2021-01-08T04:20:35.622232Z",
     "shell.execute_reply": "2021-01-08T04:20:35.622612Z"
    },
    "id": "mz-ahSdsq0Oc"
   },
   "source": [
    "#### Sub Section 4.2.3: U-Net model creation <a class=\"anchor\" id=\"section_4_2_3\"></a>\n",
    "* Activation function used for the generator is the \"tanh\" function, which outputs a value between -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:35.631772Z",
     "iopub.status.busy": "2021-01-08T04:20:35.631160Z",
     "iopub.status.idle": "2021-01-08T04:20:35.633455Z",
     "shell.execute_reply": "2021-01-08T04:20:35.632846Z"
    },
    "id": "lFPI4Nu-8b4q"
   },
   "outputs": [],
   "source": [
    "def Generator():\n",
    "  size=4\n",
    "  inputs = tf.keras.layers.Input(shape=[256,256,3])\n",
    "\n",
    "  down_stack = [\n",
    "    downsample(64, size, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
    "    downsample(128,size), # (bs, 64, 64, 128)\n",
    "    downsample(256, size), # (bs, 32, 32, 256)\n",
    "    downsample(512, size), # (bs, 16, 16, 512)\n",
    "    downsample(512, size), # (bs, 8, 8, 512)\n",
    "    downsample(512, size), # (bs, 4, 4, 512)\n",
    "    downsample(512, size), # (bs, 2, 2, 512)\n",
    "    downsample(512, size), # (bs, 1, 1, 512)\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(512, size, apply_dropout=True), # (bs, 2, 2, 1024)\n",
    "    upsample(512, size, apply_dropout=True), # (bs, 4, 4, 1024)\n",
    "    upsample(512, size, apply_dropout=True), # (bs, 8, 8, 1024)\n",
    "    upsample(512, size), # (bs, 16, 16, 1024)\n",
    "    upsample(256, size), # (bs, 32, 32, 512)\n",
    "    upsample(128, size), # (bs, 64, 64, 256)\n",
    "    upsample(64, size), # (bs, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, size,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpDPEQXIAiQO"
   },
   "source": [
    "#### Sub Section 4.2.4: Generator loss <a class=\"anchor\" id=\"section_4_2_4\"></a>\n",
    "  * It is a sigmoid cross entropy loss of the generated images and an **array of ones**.\n",
    "  * The original Pix2Pix [paper](https://arxiv.org/abs/1611.07004) also includes L1 loss which is MAE (mean absolute error) between the generated image and the target image.\n",
    "  * This allows the generated image to become structurally similar to the target image.\n",
    "  * The formula to calculate the total generator loss = gan_loss + LAMBDA * l1_loss, where LAMBDA = 100. This value was decided by the authors of the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:36.822042Z",
     "iopub.status.busy": "2021-01-08T04:20:36.821488Z",
     "iopub.status.idle": "2021-01-08T04:20:36.823067Z",
     "shell.execute_reply": "2021-01-08T04:20:36.823463Z"
    },
    "id": "90BIcCKcDMxz"
   },
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target,LAMBDA = 100):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # mean absolute error\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlB-XMY5Awj9"
   },
   "source": [
    "![Generator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/gen.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTKZfoaoEF22"
   },
   "source": [
    "### Section 4.3: Discriminator Model Creation <a class=\"anchor\" id=\"section_4_3\"></a>\n",
    "  * Discriminator receives 2 inputs.\n",
    "    * Input image and the target image, which it should classify as real.\n",
    "    * Input image and the generated image (output of generator), which it should classify as fake.\n",
    "    * These input pairs are concatenated as one tensor (`tf.concat([inp, tar], axis=-1)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 4.3.1: Model definition <a class=\"anchor\" id=\"section_4_3_1\"></a>\n",
    "  * The Discriminator is a PatchGAN.\n",
    "  * Each block in the discriminator is (Conv -> BatchNorm -> Leaky ReLU)\n",
    "  * The shape of the output after the last layer is (batch_size, 30, 30, 1)\n",
    "  * Each 30x30 patch of the output classifies a 70x70 portion of the input image (such an architecture is called a PatchGAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:36.830842Z",
     "iopub.status.busy": "2021-01-08T04:20:36.830275Z",
     "iopub.status.idle": "2021-01-08T04:20:36.831801Z",
     "shell.execute_reply": "2021-01-08T04:20:36.832146Z"
    },
    "id": "ll6aNeQx8b4v"
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  size=4\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, size, False)(x) # (bs, 128, 128, 64)\n",
    "  down2 = downsample(128, size)(down1) # (bs, 64, 64, 128)\n",
    "  down3 = downsample(256, size)(down2) # (bs, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, size, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, size, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)\n",
    "disc_out = discriminator([inp[tf.newaxis,...], gen_output], training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOqg1dhUAWoD"
   },
   "source": [
    "#### Sub Section 4.3.2: Discriminator loss definition <a class=\"anchor\" id=\"section_4_3_2\"></a>\n",
    "  * The discriminator loss function takes 2 inputs; **real images, generated images**\n",
    "  * real_loss is a sigmoid cross entropy loss of the **real images** and an **array of ones(since these are the real images)**\n",
    "  * generated_loss is a sigmoid cross entropy loss of the **generated images** and an **array of zeros(since these are the fake images)**\n",
    "  * Then the total_loss is the sum of real_loss and the generated_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.318115Z",
     "iopub.status.busy": "2021-01-08T04:20:37.317551Z",
     "iopub.status.idle": "2021-01-08T04:20:37.319661Z",
     "shell.execute_reply": "2021-01-08T04:20:37.319202Z"
    },
    "id": "wkMNfBWlT-PV"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ede4p2YELFa"
   },
   "source": [
    "The training procedure for the discriminator is shown below.\n",
    "\n",
    "To learn more about the architecture and the hyperparameters you can refer the [paper](https://arxiv.org/abs/1611.07004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IS9sHa-1BoAF"
   },
   "source": [
    "![Discriminator Update Image](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/generative/images/dis.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLKOG55MErD0"
   },
   "source": [
    "## Chapter 5: Model Training <a class=\"anchor\" id=\"chapter5\"></a>  \n",
    "\n",
    "Contains the definition of the training optimizers, main training loop and checkpoint folder definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.1: Optimizer definition <a class=\"anchor\" id=\"section_5_1\"></a>\n",
    "\n",
    "Both the generator and discriminator uses the Adam optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.323707Z",
     "iopub.status.busy": "2021-01-08T04:20:37.323167Z",
     "iopub.status.idle": "2021-01-08T04:20:37.324858Z",
     "shell.execute_reply": "2021-01-08T04:20:37.325246Z"
    },
    "id": "lbHFNexF0x6O"
   },
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.2: Checkpoint definition <a class=\"anchor\" id=\"section_5_2\"></a>\n",
    "\n",
    "This is used for storing weights and restarting training where left off. Checkpoints will be based on the name of the dataset, based on sub-section 2.2 [Go to section](#section_2_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 5.2.1: Checkpoint folder naming <a class=\"anchor\" id=\"section_5_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.647343Z",
     "iopub.status.busy": "2021-01-08T04:20:37.646775Z",
     "iopub.status.idle": "2021-01-08T04:20:37.648933Z",
     "shell.execute_reply": "2021-01-08T04:20:37.649302Z"
    },
    "id": "xNNMDBNH12q-"
   },
   "outputs": [],
   "source": [
    "# Saving Directories with different dates\n",
    "import datetime\n",
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(\n",
    "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 5.2.2: Checkpoint setup using Tensorflow functions<a class=\"anchor\" id=\"section_5_2_2\"></a>\n",
    "\n",
    "Only the most recent (up to 5 check-points) are stored \n",
    "\n",
    "* Tensorflow function to create checkpoints\n",
    "    * https://www.tensorflow.org/guide/checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.329646Z",
     "iopub.status.busy": "2021-01-08T04:20:37.329055Z",
     "iopub.status.idle": "2021-01-08T04:20:37.330724Z",
     "shell.execute_reply": "2021-01-08T04:20:37.331077Z"
    },
    "id": "WJnftd5sQsv6"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints/'+date+\"/\"+mask\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=checkpoint_prefix, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx7s-vBHFKdh"
   },
   "source": [
    "### Section 5.3: Model training <a class=\"anchor\" id=\"section_5_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 5.3.1: Training function definition <a class=\"anchor\" id=\"section_5_3_1\"></a>\n",
    "\n",
    "* The generator takes the input image (stain-deconvolved H&E image) to produce an output with CD3+ regions\n",
    "* The discriminator works twice, with two image pairs, once on the input-ground_truth pair and once on the input-predicted_image pair\n",
    "* Losses are calculated and taped back to the models for training\n",
    "* Function inputs\n",
    "   * Input image\n",
    "   * Ground-truth/target/reference image\n",
    "   * Epoch number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.656320Z",
     "iopub.status.busy": "2021-01-08T04:20:37.655689Z",
     "iopub.status.idle": "2021-01-08T04:20:37.657339Z",
     "shell.execute_reply": "2021-01-08T04:20:37.657687Z"
    },
    "id": "KBKUV2sKXDbY"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_image, target, epoch):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    \n",
    "    gen_output = generator(input_image, training=True)\n",
    "    \n",
    "\n",
    "    disc_real_output = discriminator([input_image, target], training=True)\n",
    "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "\n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n",
    "    disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "  \n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=epoch)\n",
    "    \n",
    "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=epoch)\n",
    "    \n",
    "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=epoch)\n",
    "    tf.summary.scalar('disc_loss', disc_loss, step=epoch)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 5.3.2: Tensorboard setup <a class=\"anchor\" id=\"section_5_3_2\"></a>\n",
    "\n",
    "* This application shows the progress of the model, by plotting the various losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ot22ujrlLhOd"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 5.3.3: Fit function definition <a class=\"anchor\" id=\"section_5_3_3\"></a>\n",
    "\n",
    "* The following processes are done:\n",
    "    * Running the training function\n",
    "    * Showing the progress per epoch\n",
    "    * Saving checkpoints\n",
    "* Function inputs\n",
    "    * Training dataset\n",
    "    * Number of epochs\n",
    "    * Test/Validation dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.663798Z",
     "iopub.status.busy": "2021-01-08T04:20:37.663231Z",
     "iopub.status.idle": "2021-01-08T04:20:37.665189Z",
     "shell.execute_reply": "2021-01-08T04:20:37.664728Z"
    },
    "id": "2M7LmLtGEMQJ"
   },
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        print(\"Epoch: \", epoch)\n",
    "\n",
    "    # Train\n",
    "        for n, (input_image, target) in train_ds.enumerate():\n",
    "            print('.', end='')\n",
    "   \n",
    "            train_step(input_image, target, epoch)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "                \n",
    "        manager.save()\n",
    "\n",
    "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                        time.time()-start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Section 5.3.4: Setting epochs and last check-point (if any) <a class=\"anchor\" id=\"section_5_3_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:59:43.787042Z",
     "iopub.status.busy": "2021-01-08T04:59:43.775471Z",
     "iopub.status.idle": "2021-01-08T04:59:44.376657Z",
     "shell.execute_reply": "2021-01-08T04:59:44.376098Z"
    },
    "id": "4t4x69adQ5xb"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "# restoring the latest checkpoint \n",
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_prefix))\n",
    "#Loading latest weights (preferred)\n",
    "generator.load_weights(path_gen)\n",
    "discriminator.load_weights(path_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pe0-8Bzg22ox"
   },
   "source": [
    "#### Sub Section 5.3.5: Running the \"fit\" function <a class=\"anchor\" id=\"section_5_3_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.668541Z",
     "iopub.status.busy": "2021-01-08T04:20:37.667951Z",
     "iopub.status.idle": "2021-01-08T04:59:43.521945Z",
     "shell.execute_reply": "2021-01-08T04:59:43.522342Z"
    },
    "id": "a1zZmKmvOH85",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit(train_dataset, EPOCHS, valid_dataset)\n",
    "\n",
    "#Save final weights using the save_weights function\n",
    "\n",
    "os.makedirs(path_gen,exist_ok=True)\n",
    "os.makedirs(path_dis,exist_ok=True)\n",
    "generator.save_weights(path_gen+\"/generator_weights\")\n",
    "discriminator.save_weights(path_gen+\"/discriminator_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rw1fkAczTQYh"
   },
   "source": [
    "## Chapter 6: Additional Functions <a class=\"anchor\" id=\"chapter6\"></a>\n",
    "\n",
    "A precursor on typing your own functions for the following:\n",
    "\n",
    "* Displaying generated images\n",
    "* Model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6.1: Displaying generated images <a class=\"anchor\" id=\"section_6_1\"></a>\n",
    "\n",
    "* Function inputs\n",
    "    * Generator model\n",
    "    * Input image\n",
    "    * Output image (only for comparison purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:20:37.336597Z",
     "iopub.status.busy": "2021-01-08T04:20:37.336010Z",
     "iopub.status.idle": "2021-01-08T04:20:37.337634Z",
     "shell.execute_reply": "2021-01-08T04:20:37.338041Z"
    },
    "id": "RmdVsmvhPxyy"
   },
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15,15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T04:59:44.381205Z",
     "iopub.status.busy": "2021-01-08T04:59:44.380602Z",
     "iopub.status.idle": "2021-01-08T04:59:45.738473Z",
     "shell.execute_reply": "2021-01-08T04:59:45.738911Z"
    },
    "id": "KUgSnmy2nqSP"
   },
   "source": [
    "### Section 6.2 Model inference <a class=\"anchor\" id=\"section_6_2\"></a>\n",
    "\n",
    "* Similar to the above function, except the image get stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train=filedialog.askdirectory(title='path for train_results')\n",
    "P_train_gt=os.path.join(p_train,\"gt\")\n",
    "P_train_pred=os.path.join(p_train,\"pred\")\n",
    "P_train_img=os.path.join(p_train,\"img\")\n",
    "os.makedirs(P_train_img,exist_ok=True)\n",
    "os.makedirs(P_train_gt,exist_ok=True)\n",
    "os.makedirs(P_train_pred,exist_ok=True)\n",
    "i=0\n",
    "for x,y in train_dataset:\n",
    "    for a,b in zip(x,y):\n",
    "        v=generator(np.expand_dims(a,axis=0))\n",
    "        img1=v[0]*0.5+0.5\n",
    "        img2=b*0.5+0.5\n",
    "        img3=a*0.5+0.5\n",
    "        plt.imsave(P_train_pred+\"/\"+f\"{i:06d}\"+\".png\",img1.numpy())\n",
    "        plt.imsave(P_train_gt+\"/\"+f\"{i:06d}\"+\".png\",img2.numpy())\n",
    "        plt.imsave(P_train_img+\"/\"+f\"{i:06d}\"+\".png\",img3.numpy())\n",
    "        print(i, end= \" \")\n",
    "        i=i+1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
